{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b47d8c1",
    "outputId": "786ec057-c05d-4f44-c329-15c98a60a8e0"
   },
   "outputs": [],
   "source": [
    "import torch, types\n",
    "\n",
    "# CUDA 장치의 주요 버전과 부 버전을 가져옵니다.\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "major_version, minor_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#현재깔린게 2.2임.\n",
    "import torch, triton\n",
    "print(torch.__version__)      # 2.2.2  (CUDA 12.1)\n",
    "print(triton.__version__)     # 2.2.0  ← OK\n",
    "from triton.runtime.jit import get_cuda_stream     # 오류 없으면 정상\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "- unsloth_zoo가 PyTorch ≥ 2.3에 새로 추가된 torch.amp.is_autocast_available() API를 호출합니다.\n",
    "- 현재 SageMaker 노트북의 PyTorch 2.2.2 에는 torch.amp.is_autocast_available() 함수가 없어 AttributeError가 발생\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "0c110a57"
   },
   "source": [
    "## Unsloth\n",
    "\n",
    "- `Unsloth`는 Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes 등을 지원합니다. 그리고 Yi, Qwen([llamafied](https://huggingface.co/models?sort=trending&search=qwen+llama)), Deepseek, 모든 Llama, Mistral 파생 아키텍처도 지원합니다.\n",
    "\n",
    "- `Unsloth`는 16비트 LoRA 또는 4비트 QLoRA를 지원합니다. 둘 다 2배 빠릅니다.\n",
    "\n",
    "- `max_seq_length`는 [kaiokendev의](https://kaiokendev.github.io/til) 방법을 통해 자동으로 RoPE 스케일링을 하기 때문에 어떤 값으로도 설정할 수 있습니다.\n",
    "\n",
    "**새로운 소식**!\n",
    "\n",
    "- [PR 26037](https://github.com/huggingface/transformers/pull/26037)을 통해, 우리는 4비트 모델을 **4배 빠르게** 다운로드할 수 있는 기능을 지원합니다! [Unsloth Repository](https://huggingface.co/unsloth)에는 Llama, Mistral 4비트 모델이 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "79e77752"
   },
   "source": [
    "`FastLanguageModel.from_pretrained` 함수를 사용하여 사전 훈련된 언어 모델을 로드하는 과정을 설명합니다.\n",
    "\n",
    "- 최대 시퀀스 길이(`max_seq_length`)를 설정하여 모델이 처리할 수 있는 입력 데이터의 길이를 지정합니다.\n",
    "- 데이터 타입(`dtype`)은 자동 감지되거나, 특정 하드웨어에 최적화된 형식(`Float16`, `Bfloat16`)으로 설정할 수 있습니다.\n",
    "- 4비트 양자화(`load_in_4bit`) 옵션을 사용하여 메모리 사용량을 줄일 수 있으며, 이는 선택적입니다.\n",
    "- 사전 정의된 4비트 양자화 모델 목록(`fourbit_models`)에서 선택하여 다운로드 시간을 단축하고 메모리 부족 문제를 방지할 수 있습니다.\n",
    "- `FastLanguageModel.from_pretrained` 함수를 통해 모델과 토크나이저를 로드하며, 이때 모델 이름(`model_name`), 최대 시퀀스 길이, 데이터 타입, 4비트 로딩 여부를 매개변수로 전달합니다.\n",
    "- 선택적으로, 특정 게이트 모델을 사용할 경우 토큰(`token`)을 제공할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608,
     "referenced_widgets": [
      "c82b631ec4b34eccb3a88dd69cbeb2a7",
      "0290d2187c07417d9302ab54743da2dc",
      "e26b5300257343fa8de572476889a307",
      "fc50ec0bde054cd5b0630a81c48a0bea",
      "08c78aff0caa402da3740d4bc988267a",
      "a266917c9fab4b21a9f4ce29da89f7ed",
      "847fe79e33994984be9595968751f326",
      "8a52d1497a934d22a6ea073370425923",
      "85efd95e4f00493d94624bb706117a03",
      "86f7efb842744ee8a51753689dcbbbd6",
      "b95095d65b704679840eb5af96e5d6b0",
      "b9892219d73f4696b5471e73f8b6dccb",
      "38849ecba0884da4a698fde06d91b13e",
      "997861b04cf647e9a6bd727b318be47a",
      "26ed41293600404c926b0884aabadfd5",
      "ef8ef08dd7284655a29ee567061c7a32",
      "e6690121b11a48b0a8c783df1cbfabbb",
      "d2c5b80c3b75464db9a34d01aa37950e",
      "6741a04cf7594497873d7cc5c1832a2f",
      "95cadd60850843049dd91fd64d7c95e1",
      "f862a90a580944b9b5c5ecb597f972b5",
      "c27a27d1c8834df6af1c309b0fcdabcb",
      "df0d196eecc74b34b5504f6830805fd2",
      "a3082881f419403b807007728c8a117e",
      "5641155be7f947309112a36c48012799",
      "44d18bd62fb145c1acab537f02b5a60b",
      "90936a2c7fd44e1aace25772b6f4f217",
      "dd27c8c888ee4dbbb95021b537d52e63",
      "7118e70b23aa426d9b5ad2fc922fd1f0",
      "3cb52fcabe9b4d21925dcf942816a827",
      "55941ea7b03f46c28d349fe28bfc8716",
      "e1426915a6d74f1983445cbba14a8a12",
      "fed268fca78e41afbaf2be34971c2668",
      "bff46daf2e1b40bb941c991ab258cbeb",
      "4be2ac8988a1470f87489ed571f5eaa9",
      "80aa72d7797c44fa978ded5071d301f2",
      "ca314337bf7c4c698fd53f36a23cb710",
      "453308addf72430c9c63a16d0f060e5b",
      "56d6a5e55ad246398c49cc5565448799",
      "006aa3df063c4521ac5f832fe1307c85",
      "52838461d16a4aa8b9806f379089cc78",
      "5f01c958d62f4ce58cdd818bb3b22973",
      "89530754ffb54aea80cc15d3fe05c461",
      "c4475b99355e4905a67fe11f6f47247b",
      "10ca0ddfebf745168c12f4dc6e5fef06",
      "ac2885b19eff4496bb085eaa9fcc8326",
      "a7baf821c0594e888b5dfbe1cfeec917",
      "5b6eaa201e654366bca96093b2959440",
      "5fcb8be821b8440d9a8a31bbbf1af48b",
      "0f9451dc85ba4044aa763f7349e69ad0",
      "ca3dac1f5f2b40fb8e4b30d172f3579a",
      "b9591f8111984873abd2e3f6b5653f96",
      "acc5ca922af84ad095cb72552e0476ef",
      "5e0781040f3a436580dd6072ce5aa75e",
      "508bb6ab2bf74e4a9ed57a0371180e17",
      "5c58b7a09fd14a0f994605b73e685e16",
      "6fec1d6b82c4440e81e4ff7748549392",
      "8dd7421380e64b86aada4e7b4a09b15b",
      "eaa2b1d6b5a64c2a8050a7888b3c4648",
      "fce99cbf870c41ffa3460cd8f6fd665e",
      "0e62afd9b85a4004878c63dfc26c641f",
      "c1c0ab070f2448fe804693456aa6c679",
      "9e3e3179c27e42698c442afaca46eb05",
      "ede421717dd5461183db3a6af60d40f3",
      "59de36b98d12457182bb85fada4e53ed",
      "fafdec94b3e147bba0d999342e98f550",
      "5cb65bde9e8242e38aa98862e05208a0",
      "eb0549767ae94f649a9cbb15abe61bc9",
      "62e623c653c6413dba5cf0f0f24b4faf",
      "58034845c96348bc89df363fcf344da6",
      "ab2c635efd8644a985b6720f5a102c96",
      "25efba438e33473da11ac3a7e3a793e6",
      "100b019c5a214cb8b85b7f5ae2119d5d",
      "197a694dd64d41348475168098abb3ab",
      "be8eba78342c44bf90bfb62b68a4bf09",
      "f9367da780a94ae2860863852c9515bc",
      "f686ebfdfa0e452081e44a191017aab6",
      "66e279d8f1ff4be9a3892cee6e700959",
      "f216321cede6408ba0ff90c0b52ece4f",
      "8813817abcf7487e8b83a7f5c6055f95",
      "f2d75fc6327243208c3740332d054637",
      "0e4b3f9b418c470c8e3a7e939e9c40c3",
      "ba00e694bbba41f5b8045352ad057f97",
      "c40e2bc874c34305bc5d8920310f7d94",
      "aae8d86cb83c45d4a15e9ec768ce6de4",
      "04576ed4663c4754bfc64ee8bd60898e",
      "215c69a650fa447e90aa977a24fb4dc7",
      "9e91d0517db54c74bfe980cebdbb4900",
      "1c1c2d5fbc314f70a120718ff1569241",
      "2982cc146cb747019135164113b63879",
      "064a3daa88a54a96b1275c704b6625d1",
      "33dc62ca03074c52ace954181c28a5f0",
      "5e57ccf6d4064e12be5faf377a51b565",
      "c6d17daa45624f0a9639409b47b94924",
      "5a7a205df38d4adb97875944c5dbdca3",
      "535b6b46bdb54e9dabe5e6e23a40d346",
      "7456cc527a9645068e91bb9e805bbd7e",
      "39bfc699ad3d4e73aabe1234a7ad1573",
      "dddc259c023b4fd88ee640e6d9730e38",
      "81fb844fe2c14e29897ea5b37e672adc",
      "2324c9923fd046a0bc6580e901b92dad",
      "d4d8c992f3364bb5bbfa3c55d0dfb593",
      "83c0fd99eec54214b424512334a5e4a6",
      "900c7b7d5a3b4574bb54a47233bbdaea",
      "c54491ff396146e0999a180e5be4aebf",
      "23f0ea83d72e4ab59d539a567eb20a36",
      "73b78381a5d54cf098a695503727c0cb",
      "f856a6738c404995b68277678818fb7e",
      "1d9cdf6079b648b4a1b8eeb0cea1fa43",
      "2ebbc11da0d54aabacb4add5300b0d08",
      "8b6feda602a040a482cba439f613e7a8",
      "9d9f97aa8106466299752fecd1e7e2d6",
      "6f4f7d14c61b446a851cd93bd17ed003",
      "30ca7c425216497d88967d62d83bc9fd",
      "0d9335d8b2564a518df75d3d0ec8860b",
      "a9bb6750e657475d9373fbc0e7fc85d9",
      "4fe58c1d715748ce8c90e9567a9e69be",
      "2a41774120f94fc2a0a49d467dad9897",
      "4cda3126d0e3428b865dc24d7ae60f2e",
      "a6829eb4b3e64d2caff44b932e63b4c4",
      "d6a2bd5bb29a40528c4d660efdb23e11",
      "9db627ce367242e7a374a14d59419aac",
      "f86f4fd1cc934cdbbfe44f84168646ab",
      "a93dc092082244049dd0589d90cc5d0b",
      "ae40412ec12249719f1a0bcb2650cff4",
      "32526257e6774c15a88f908e4ce3a8f4",
      "5fd4ca07fc894afcbcf55e29809c40d1",
      "f3c41be2d6b54ac68d873432478f6b82",
      "6c5911e019e447ccba3405de971ae416",
      "9e5a03ea3cf9446b8341978d942f7677",
      "37ab0e207b5245608ca7c58b566da4a8",
      "cf8b9e484f29457aa723980d22d1f9db",
      "509f02d8d0f043a9a7eedc4e24fe29c7",
      "ee734f9997e24f64a1a6aec542662fbc",
      "7d810c23b32849e9b38f8b1ad944adbf",
      "f8bbb09e33a84fc4ae2777d08368224f",
      "e9911df32739492c9ae2b7202e5c5899",
      "f4a3cb640ebc42d4a104329f878889a4",
      "1658c20eb4244ce7b578bd4bd52a141f",
      "75ba0f0586e44a7ebb6d67a1dfa75995",
      "0d35ac03b2914b97b1c58d7a24afbca7",
      "b6d5862c271c4118a72b067d53fb2343",
      "6939a02cb7ab40f08486b38f41490aae",
      "2aadcbfcf79840a6ad135b29cb4c9e57",
      "257aad534fcd4bfd9b2a795d54695645",
      "17d5e64c2e0a49aeb88e22746b3ecbac",
      "d9bca499d3674b57bbc50b89ff490d8c",
      "1661a129f1e146828f9003b5b73a247c",
      "241908b8940142dfbd2673ba6abb27a5",
      "1b83a29252b14532867f02c9376cbdc5",
      "9f38bc1f9d81458e9975e67e441da8cc",
      "22d658a6e600477cb704807c10c6408f",
      "156bdbb90bd9404c82751087ae709fae",
      "38224f13f2cb4bb9a315f7e1a6140cb6"
     ]
    },
    "id": "ebaf5ef0",
    "outputId": "2407a1de-5bb4-41e6-e660-202b4d6645db"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096  # 최대 시퀀스 길이를 설정합니다. 내부적으로 RoPE 스케일링을 자동으로 지원합니다!\n",
    "# 자동 감지를 위해 None을 사용합니다. Tesla T4, V100은 Float16, Ampere+는 Bfloat16을 사용하세요.\n",
    "dtype = None\n",
    "# 메모리 사용량을 줄이기 위해 4bit 양자화를 사용합니다. False일 수도 있습니다.\n",
    "load_in_4bit = True\n",
    "\n",
    "# 4배 빠른 다운로드와 메모리 부족 문제를 방지하기 위해 지원하는 4bit 사전 양자화 모델입니다.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",  # Gemma 7b의 Instruct 버전\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",  # Gemma 2b의 Instruct 버전\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",  # Llama-3 8B\n",
    "]  # 더 많은 모델은 https://huggingface.co/unsloth 에서 확인할 수 있습니다.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    model_name=\"beomi/Llama-3-Open-Ko-8B-Instruct-preview\",  # 모델 이름을 설정합니다.\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이를 설정합니다.\n",
    "    dtype=dtype,  # 데이터 타입을 설정합니다.\n",
    "    load_in_4bit=load_in_4bit,  # 4bit 양자화 로드 여부를 설정합니다.\n",
    "    # token = \"hf_...\", # 게이트된 모델을 사용하는 경우 토큰을 사용하세요. 예: meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "2c0fe045"
   },
   "source": [
    "이제 LoRA 어댑터를 추가하여 모든 파라미터 중 단 1% ~ 10%의 파라미터만 업데이트하면 됩니다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "58c719fd"
   },
   "source": [
    "FastLanguageModel을 사용하여 특정 모듈에 대한 성능 향상 기법을 적용한 모델을 구성합니다.\n",
    "\n",
    "- `FastLanguageModel.get_peft_model` 함수를 호출하여 모델을 초기화하고, 성능 향상을 위한 여러 파라미터를 설정합니다.\n",
    "- `r` 파라미터를 통해 성능 향상 기법의 강도를 조절합니다. 권장 값으로는 8, 16, 32, 64, 128 등이 있습니다.\n",
    "- `target_modules` 리스트에는 성능 향상을 적용할 모델의 모듈 이름들이 포함됩니다.\n",
    "- `lora_alpha`와 `lora_dropout`을 설정하여 LoRA(Low-Rank Adaptation) 기법의 세부 파라미터를 조정합니다.\n",
    "- `bias` 옵션을 통해 모델의 바이어스 사용 여부를 설정할 수 있으며, 최적화를 위해 \"none\"으로 설정하는 것이 권장됩니다.\n",
    "- `use_gradient_checkpointing` 옵션을 \"unsloth\"로 설정하여 VRAM 사용량을 줄이고, 더 큰 배치 크기로 학습할 수 있도록 합니다.\n",
    "- `use_rslora` 옵션을 통해 Rank Stabilized LoRA를 사용할지 여부를 결정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f990387e",
    "outputId": "663b2111-d856-4e9c-999a-debdc08ca49e"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.\n",
    "    lora_alpha=32,  # LoRA 알파 값을 설정합니다.\n",
    "    lora_dropout=0.05,  # 드롭아웃을 지원합니다.\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # 타겟 모듈을 지정합니다.\n",
    "    bias=\"none\",  # 바이어스를 지원합니다.\n",
    "    # True 또는 \"unsloth\"를 사용하여 매우 긴 컨텍스트에 대해 VRAM을 30% 덜 사용하고, 2배 더 큰 배치 크기를 지원합니다.\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=123,  # 난수 상태를 설정합니다.\n",
    "    use_rslora=False,  # 순위 안정화 LoRA를 지원합니다.\n",
    "    loftq_config=None,  # LoftQ를 지원합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "1cd96e8b"
   },
   "source": [
    "### 데이터 준비\n",
    "\n",
    "**[중요]**\n",
    "\n",
    "- 토큰화된 출력에 **EOS_TOKEN**을 추가하는 것을 잊지 마세요! 그렇지 않으면 무한 생성이 발생할 수 있습니다.\n",
    "\n",
    "**[참고]**\n",
    "\n",
    "- 오직 완성된 텍스트만을 학습하고자 한다면, TRL의 문서를 [여기](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)에서 확인하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "7dd387ec"
   },
   "source": [
    "`load_dataset` 함수를 사용하여 특정 데이터셋을 로드하고, 이를 특정 형식으로 포매팅하는 과정을 설명합니다.\n",
    "\n",
    "- `load_dataset` 함수로 \"teddylee777/QA-Dataset-mini\" 데이터셋을 \"train\" 분할로 로드합니다.\n",
    "- 데이터셋의 각 예제에 대해 `formatting_prompts_func` 함수를 적용하여 포매팅을 수행합니다.\n",
    "  - 이 함수는 \"instruction\"과 \"output\" 필드를 사용하여 주어진 포맷에 맞게 텍스트를 재구성합니다.\n",
    "  - 재구성된 텍스트는 `alpaca_prompt` 포맷을 따르며, 각 항목의 끝에는 `EOS_TOKEN`을 추가하여 생성이 종료되도록 합니다.\n",
    "- 최종적으로, 포매팅된 텍스트는 \"text\" 키를 가진 딕셔너리 형태로 반환됩니다.\n",
    "- 이 과정을 통해, AI 모델이 처리하기 적합한 형태로 데이터를 전처리하는 방법을 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1051a18ef1a94646a330d1f42b33afc4",
      "3c0f97331ac54e4096419842d32e4e7e",
      "d23634e1e21941c6a4631e1c24b9542e",
      "694947e5188942d1b2d112d82177d1b0",
      "2ae31dabcf524a3ca132fb6a6f6c52d0",
      "e2bf55927a7d433fada252635594dff5",
      "0683156f8cee42b38ccf22a3522461ec",
      "8f7d53d84b1b4884b300e95ef65b251a",
      "872f2ce0bc8745d89d2b0dfc76cebeda",
      "7238acef7a784aec9ac2ad6942c34ff0",
      "6ba81f98392e47a1bc840362e1c56939",
      "0c35cb640bec45b3beb10e7c36ec41ae",
      "f38b82466bff4e9e9691717840b08f81",
      "fe145b3cf6cb4916aec92c855ac6f5cc",
      "8bc57e7f304f475ba344b4d44857c581",
      "e75e8e176da44cafbac169630481269e",
      "343d5b87f7e846258d1c4242ae979a07",
      "3db5f7d69a6e41e98d4bcc67b92f1ff5",
      "14e2b8cd41764e2b9f0a2a75c5e7c22c",
      "7ec19800bab64e3db16c132439dc01fe",
      "d49c61f792284b1f92b421b9f79d3ad1",
      "217ab9f097954ba5975bff834f78a597",
      "1320c8b175874ceab90b3080c055578b",
      "03f2d478a6f04a4ba73a1ab0bfbfcbe9",
      "51a00fa35cf642fd8090298cf82fbe28",
      "9bff62df789f4675a2790a4708558e71",
      "c2c74e7cc99a40d69bc66a2ca0735dec",
      "d67ce9b974ad4a3987782906ae9ae0fc",
      "84c11a93eec642ee84549e837e9564f5",
      "f89378aef63148b290f9c33b36f0265f",
      "f818588783354dffa838660e33367672",
      "4ea84d4aeddb418987748906f8d89104",
      "e6c1e674d7794cd1ab8294042ac0dfb1",
      "142b69ecaa84447b9e3931c16df041f8",
      "d965b86b78ba46b5bafcfd853c5a1bed",
      "104533e332514cfba5fb43fa4f7d0dbf",
      "044656c5431746e2afa416c0ca91f3ad",
      "4e8c7b53ec5843a5ad18b7fdfdfcdca6",
      "4f2654b42fc84943ba4ea5815a1baa3c",
      "480a9a6f262d43eeb69cd218fb9ab5d6",
      "087a0d727269459198204a6fdca7b445",
      "a62b362e056c4bd2862517e804e22214",
      "059c48277af144f796a94f94e1a9c814",
      "8c654210b9674b959b9570f6d5addbf5"
     ]
    },
    "id": "1eb5d1bf",
    "outputId": "88da0b45-853b-4b14-e95f-6f1019e08390"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# EOS_TOKEN은 문장의 끝을 나타내는 토큰입니다. 이 토큰을 추가해야 합니다.\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Prompt를 사용하여 지시사항을 포맷팅하는 함수입니다.\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "# 주어진 예시들을 포맷팅하는 함수입니다.\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]  # 지시사항을 가져옵니다.\n",
    "    outputs = examples[\"output\"]  # 출력값을 가져옵니다.\n",
    "    texts = []  # 포맷팅된 텍스트를 저장할 리스트입니다.\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        # EOS_TOKEN을 추가해야 합니다. 그렇지 않으면 생성이 무한히 진행될 수 있습니다.\n",
    "        text = prompt.format(instruction, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,  # 포맷팅된 텍스트를 반환합니다.\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 JSONL 파일 로드\n",
    "dataset = load_dataset('json', data_files='data/train_data.jsonl', split='train')\n",
    "\n",
    "# 데이터셋에 formatting_prompts_func 함수를 적용합니다. 배치 처리를 활성화합니다.\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "4c290b06"
   },
   "source": [
    "### 모델 훈련하기\n",
    "\n",
    "이제 Huggingface TRL의 `SFTTrainer`를 사용해 봅시다!\n",
    "\n",
    "- 참고 문서: [TRL SFT 문서](https://huggingface.co/docs/trl/sft_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123,
     "referenced_widgets": [
      "ebf0af95b9fc4bb88375dde9df666bb5",
      "9ead35f0817c44e9bd32dc6114ea2eaf",
      "80cf3bbe986049a99fb59772545d12e7",
      "73dabe4081f845e4ac4745c36acf1479",
      "ab9e8aec9cc3469dab2ab1471c90bc8d",
      "8464366c6695491e8527fa649628def1",
      "529c6085364d452fb974758ec6d3d22a",
      "e6877dd00cce4b469fde96f2394069bb",
      "e699c41d7d8547058ced67acbf97da7e",
      "16099d5666cb4765af04f65b9fe6842a",
      "9c0d336467d24b359efd024d204b8e8e"
     ]
    },
    "id": "b41b8fd9",
    "outputId": "173f3598-b388-4f1b-98b6-55d7d9ff554d"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # 토크나이저의 패딩을 오른쪽으로 설정합니다.\n",
    "\n",
    "# SFTTrainer를 사용하여 모델 학습 설정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # 학습할 모델\n",
    "    tokenizer=tokenizer,  # 토크나이저\n",
    "    train_dataset=dataset,  # 학습 데이터셋\n",
    "    eval_dataset=dataset,\n",
    "    dataset_text_field=\"text\",  # 데이터셋에서 텍스트 필드의 이름\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이\n",
    "    dataset_num_proc=2,  # 데이터 처리에 사용할 프로세스 수\n",
    "    packing=False,  # 짧은 시퀀스에 대한 학습 속도를 5배 빠르게 할 수 있음\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,  # 각 디바이스당 훈련 배치 크기\n",
    "        gradient_accumulation_steps=4,  # 그래디언트 누적 단계\n",
    "        warmup_steps=5,  # 웜업 스텝 수\n",
    "        num_train_epochs=3,  # 훈련 에폭 수\n",
    "        max_steps=100,  # 최대 스텝 수\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        logging_steps=1,  # logging 스텝 수\n",
    "        learning_rate=2e-4,  # 학습률\n",
    "        fp16=not torch.cuda.is_bf16_supported(),  # fp16 사용 여부, bf16이 지원되지 않는 경우에만 사용\n",
    "        bf16=torch.cuda.is_bf16_supported(),  # bf16 사용 여부, bf16이 지원되는 경우에만 사용\n",
    "        optim=\"adamw_8bit\",  # 최적화 알고리즘\n",
    "        weight_decay=0.01,  # 가중치 감소\n",
    "        lr_scheduler_type=\"cosine\",  # 학습률 스케줄러 유형\n",
    "        seed=123,  # 랜덤 시드\n",
    "        output_dir=\"outputs\",  # 출력 디렉토리\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "a7e10974"
   },
   "source": [
    "- GPU의 현재 메모리 상태를 확인합니다.\n",
    "- `torch.cuda.get_device_properties(0)`를 사용하여 첫 번째 GPU의 속성을 조회합니다.\n",
    "- `torch.cuda.max_memory_reserved()`를 통해 현재 예약된 최대 메모리를 GB 단위로 계산합니다.\n",
    "- GPU의 총 메모리 크기를 GB 단위로 계산합니다.\n",
    "- GPU 이름과 최대 메모리 크기, 현재 예약된 메모리 크기를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b91c701f",
    "outputId": "2f6e197b-f8e0-451f-e498-b21d6b581c3d"
   },
   "outputs": [],
   "source": [
    "# 현재 메모리 상태를 보여주는 코드\n",
    "gpu_stats = torch.cuda.get_device_properties(0)  # GPU 속성 가져오기\n",
    "start_gpu_memory = round(\n",
    "    torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3\n",
    ")  # 시작 시 예약된 GPU 메모리 계산\n",
    "max_memory = round(\n",
    "    gpu_stats.total_memory / 1024 / 1024 / 1024, 3\n",
    ")  # GPU의 최대 메모리 계산\n",
    "print(\n",
    "    f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\"\n",
    ")  # GPU 이름과 최대 메모리 출력\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")  # 예약된 메모리 양 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monkey patch 패치 적용\n",
    "import torch.amp\n",
    "import contextlib\n",
    "\n",
    "# is_autocast_available 함수 추가\n",
    "if not hasattr(torch.amp, 'is_autocast_available'):\n",
    "    def is_autocast_available(device_type):\n",
    "        if device_type == 'cuda':\n",
    "            return True\n",
    "        elif device_type == 'cpu':\n",
    "            return hasattr(torch.cpu, 'amp') and hasattr(torch.cpu.amp, 'autocast')\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    torch.amp.is_autocast_available = is_autocast_available\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "91622afd",
    "outputId": "f12e2b79-d75e-4b1b-a7c6-0ba3dc065818"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()  # 모델을 훈련시키고 통계를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "ac245967"
   },
   "source": [
    "PyTorch를 사용하여 모델 훈련 시 메모리 사용량과 훈련 시간을 계산하고 출력하는 코드입니다.\n",
    "\n",
    "- `torch.cuda.max_memory_reserved()`를 사용하여 훈련 중에 예약된 최대 GPU 메모리를 계산합니다.\n",
    "- 훈련 시작 시점의 GPU 메모리 사용량과 비교하여 LoRA(Low-Rank Adaptation)를 위해 사용된 추가 메모리 양을 계산합니다.\n",
    "- 전체 GPU 메모리 대비 사용된 메모리의 비율을 계산합니다.\n",
    "- 훈련에 소요된 총 시간을 초와 분 단위로 출력합니다.\n",
    "- 예약된 최대 메모리, LoRA를 위해 사용된 메모리, 그리고 이들이 전체 GPU 메모리 대비 차지하는 비율을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f17487f",
    "outputId": "c3cc7749-a144-49b2-a5ab-0a0994193f8b"
   },
   "outputs": [],
   "source": [
    "# 최종 메모리 및 시간 통계를 보여줍니다.\n",
    "used_memory = round(\n",
    "    torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3\n",
    ")  # 사용된 최대 메모리를 GB 단위로 계산합니다.\n",
    "used_memory_for_lora = round(\n",
    "    used_memory - start_gpu_memory, 3\n",
    ")  # LoRA를 위해 사용된 메모리를 GB 단위로 계산합니다.\n",
    "used_percentage = round(\n",
    "    used_memory / max_memory * 100, 3\n",
    ")  # 최대 메모리 대비 사용된 메모리의 비율을 계산합니다.\n",
    "lora_percentage = round(\n",
    "    used_memory_for_lora / max_memory * 100, 3\n",
    ")  # 최대 메모리 대비 LoRA를 위해 사용된 메모리의 비율을 계산합니다.\n",
    "print(\n",
    "    f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\"\n",
    ")  # 훈련에 사용된 시간을 초 단위로 출력합니다.\n",
    "print(\n",
    "    # 훈련에 사용된 시간을 분 단위로 출력합니다.\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(\n",
    "    f\"Peak reserved memory = {used_memory} GB.\"\n",
    ")  # 예약된 최대 메모리를 GB 단위로 출력합니다.\n",
    "print(\n",
    "    f\"Peak reserved memory for training = {used_memory_for_lora} GB.\"\n",
    ")  # 훈련을 위해 예약된 최대 메모리를 GB 단위로 출력합니다.\n",
    "print(\n",
    "    f\"Peak reserved memory % of max memory = {used_percentage} %.\"\n",
    ")  # 최대 메모리 대비 예약된 메모리의 비율을 출력합니다.\n",
    "print(\n",
    "    f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\"\n",
    ")  # 최대 메모리 대비 훈련을 위해 예약된 메모리의 비율을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "bdf87c13"
   },
   "source": [
    "### 추론\n",
    "\n",
    "모델을 실행해 봅시다! 지시사항과 입력값을 변경할 수 있으며, 출력값은 비워두세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "605d8d21"
   },
   "source": [
    "`TextStreamer`를 사용하여 연속적인 추론을 수행할 수도 있습니다. 이를 통해 전체를 기다리는 대신 토큰별로 생성 결과를 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "71019b2a"
   },
   "source": [
    "- `FastLanguageModel.for_inference(model)`을 호출하여 모델의 추론 속도를 2배 향상시킵니다.\n",
    "- `tokenizer`를 사용하여 특정 포맷의 프롬프트를 토큰화하고, 이를 CUDA 기반의 텐서로 변환합니다. 이 과정에서 피보나치 수열을 계속하는 지시문, 입력값, 그리고 출력값을 위한 빈 공간을 포함합니다.\n",
    "- `TextStreamer` 객체를 `tokenizer`와 함께 초기화하여 텍스트 스트리밍 기능을 활성화합니다.\n",
    "- `model.generate` 함수를 호출하여 주어진 입력에 대한 텍스트 생성을 수행합니다. 이때, 최대 128개의 새로운 토큰을 생성할 수 있도록 설정하고, `TextStreamer`를 사용하여 결과를 스트리밍합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "d58adce5"
   },
   "source": [
    "`StoppingCriteria`와 `StoppingCriteriaList`를 사용하여 특정 토큰에서 생성을 중단하는 방법을 구현합니다.\n",
    "\n",
    "- `StopOnToken` 클래스는 `StoppingCriteria`를 상속받아, 생성 중 특정 토큰(`stop_token_id`)이 나타나면 생성을 중단하도록 합니다.\n",
    "- `stop_token` 변수에 중단할 토큰을 문자열로 지정합니다.\n",
    "- `tokenizer.encode` 메소드를 사용하여 `stop_token`을 해당 언어 모델의 토큰 ID로 변환합니다.\n",
    "- `StoppingCriteriaList`에 `StopOnToken` 인스턴스를 포함시켜, 생성 과정에서 이를 중단 조건으로 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "bbfd6d48"
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id  # 정지 토큰 ID를 초기화합니다.\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return (\n",
    "            self.stop_token_id in input_ids[0]\n",
    "        )  # 입력된 ID 중 정지 토큰 ID가 있으면 정지합니다.\n",
    "\n",
    "\n",
    "# end_token을 설정\n",
    "stop_token = \"<|end_of_text|>\"  # end_token으로 사용할 토큰을 설정합니다.\n",
    "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[\n",
    "    0\n",
    "]  # end_token의 ID를 인코딩합니다.\n",
    "\n",
    "# Stopping criteria 설정\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [StopOnToken(stop_token_id)]\n",
    ")  # 정지 조건을 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "_ZGu2HntqpdN"
   },
   "source": [
    "(예시 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f8441b0",
    "outputId": "380dc31c-4d36-4ee0-a32e-891ec36236aa"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# FastLanguageModel을 이용하여 추론 속도를 2배 빠르게 설정합니다.\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"전현상은 누구입니까?\",  # 지시사항\n",
    "            \"\",  # 출력 - 생성을 위해 이 부분을 비워둡니다!\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=1024,  # 최대 생성 토큰 수를 설정합니다.\n",
    "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# FastLanguageModel을 이용하여 추론 속도를 2배 빠르게 설정합니다.\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"금융보안교육센터의 주소?\",  # 지시사항\n",
    "            \"\",  # 출력 - 생성을 위해 이 부분을 비워둡니다!\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=1024,  # 최대 생성 토큰 수를 설정합니다.\n",
    "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "emWg-dCBqjZD"
   },
   "source": [
    "(예시2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
