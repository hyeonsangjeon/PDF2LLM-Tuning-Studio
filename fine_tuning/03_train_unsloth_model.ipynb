{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Unsloth를 활용한 LLM 파인튜닝\n",
    "\n",
    "이 노트북에서는 Unsloth 라이브러리를 사용하여 PDF Q&A 데이터로 LLM을 효율적으로 파인튜닝하는 방법을 설명합니다.\n",
    "\n",
    "## 1. 환경 설정 및 라이브러리 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup script for Triton runtime with CUDA support\n",
    "# This script checks if the Triton runtime is correctly set up with CUDA support.\n",
    "# 필수 라이브러리 임포트 및 버전 확인\n",
    "\n",
    "import torch, triton, flash_attn, trl\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")      # PyTorch 2.2.2 (CUDA 12.1)\n",
    "print(f\"Triton 버전: {triton.__version__}\")      # 2.2.0\n",
    "print(f\"Flash-Attention 버전: {flash_attn.__version__}\")\n",
    "print(f\"TRL 버전: {trl.__version__}\") \n",
    "\n",
    "# Triton 기능 테스트\n",
    "from triton.runtime.jit import get_cuda_stream     # 오류 없으면 정상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956b55e",
   "metadata": {},
   "source": [
    "## 2. 모델 로드 및 설정\n",
    "\n",
    "- `Unsloth`는 Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes 등을 지원합니다. 그리고 Yi, Qwen([llamafied](https://huggingface.co/models?sort=trending&search=qwen+llama)), Deepseek, 모든 Llama, Mistral 파생 아키텍처도 지원합니다.\n",
    "\n",
    "- 사용방법 및 더 많은 양자화 모델 카탈로그 참조: https://huggingface.co/unsloth\n",
    "\n",
    "Unsloth는 Llama, Mistral, Gemma 등 다양한 모델을 지원하며, 다양한 양자화와 LoRA를 통해 메모리 사용량을 크게 줄이고 훈련 속도를 높입니다.\n",
    "\n",
    "### `FastLanguageModel.from_pretrained` 모델 로딩 가이드\n",
    "\n",
    "### 핵심 파라미터\n",
    "- **모델명(`model_name`)**: 사전 훈련된 모델 지정\n",
    "- **최대 시퀀스 길이(`max_seq_length`)**: 처리할 입력 데이터의 최대 길이\n",
    "- **데이터 타입(`dtype`)**: 자동 감지 또는 `Float16`/`Bfloat16` 지정 가능\n",
    "- **양자화(`True`)**: 메모리 사용량 감소를 위한 선택적 옵션\n",
    "\n",
    "### 최적화 옵션\n",
    "- 사전 정의된 4비트 모델 목록(`fourbit_models`)을 활용하면 다운로드 시간 단축 및 메모리 효율성 향상\n",
    "- 특정 게이트 모델 사용 시 `token` 파라미터로 액세스 권한 제공 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608,
     "referenced_widgets": [
      "c82b631ec4b34eccb3a88dd69cbeb2a7",
      "0290d2187c07417d9302ab54743da2dc",
      "e26b5300257343fa8de572476889a307",
      "fc50ec0bde054cd5b0630a81c48a0bea",
      "08c78aff0caa402da3740d4bc988267a",
      "a266917c9fab4b21a9f4ce29da89f7ed",
      "847fe79e33994984be9595968751f326",
      "8a52d1497a934d22a6ea073370425923",
      "85efd95e4f00493d94624bb706117a03",
      "86f7efb842744ee8a51753689dcbbbd6",
      "b95095d65b704679840eb5af96e5d6b0",
      "b9892219d73f4696b5471e73f8b6dccb",
      "38849ecba0884da4a698fde06d91b13e",
      "997861b04cf647e9a6bd727b318be47a",
      "26ed41293600404c926b0884aabadfd5",
      "ef8ef08dd7284655a29ee567061c7a32",
      "e6690121b11a48b0a8c783df1cbfabbb",
      "d2c5b80c3b75464db9a34d01aa37950e",
      "6741a04cf7594497873d7cc5c1832a2f",
      "95cadd60850843049dd91fd64d7c95e1",
      "f862a90a580944b9b5c5ecb597f972b5",
      "c27a27d1c8834df6af1c309b0fcdabcb",
      "df0d196eecc74b34b5504f6830805fd2",
      "a3082881f419403b807007728c8a117e",
      "5641155be7f947309112a36c48012799",
      "44d18bd62fb145c1acab537f02b5a60b",
      "90936a2c7fd44e1aace25772b6f4f217",
      "dd27c8c888ee4dbbb95021b537d52e63",
      "7118e70b23aa426d9b5ad2fc922fd1f0",
      "3cb52fcabe9b4d21925dcf942816a827",
      "55941ea7b03f46c28d349fe28bfc8716",
      "e1426915a6d74f1983445cbba14a8a12",
      "fed268fca78e41afbaf2be34971c2668",
      "bff46daf2e1b40bb941c991ab258cbeb",
      "4be2ac8988a1470f87489ed571f5eaa9",
      "80aa72d7797c44fa978ded5071d301f2",
      "ca314337bf7c4c698fd53f36a23cb710",
      "453308addf72430c9c63a16d0f060e5b",
      "56d6a5e55ad246398c49cc5565448799",
      "006aa3df063c4521ac5f832fe1307c85",
      "52838461d16a4aa8b9806f379089cc78",
      "5f01c958d62f4ce58cdd818bb3b22973",
      "89530754ffb54aea80cc15d3fe05c461",
      "c4475b99355e4905a67fe11f6f47247b",
      "10ca0ddfebf745168c12f4dc6e5fef06",
      "ac2885b19eff4496bb085eaa9fcc8326",
      "a7baf821c0594e888b5dfbe1cfeec917",
      "5b6eaa201e654366bca96093b2959440",
      "5fcb8be821b8440d9a8a31bbbf1af48b",
      "0f9451dc85ba4044aa763f7349e69ad0",
      "ca3dac1f5f2b40fb8e4b30d172f3579a",
      "b9591f8111984873abd2e3f6b5653f96",
      "acc5ca922af84ad095cb72552e0476ef",
      "5e0781040f3a436580dd6072ce5aa75e",
      "508bb6ab2bf74e4a9ed57a0371180e17",
      "5c58b7a09fd14a0f994605b73e685e16",
      "6fec1d6b82c4440e81e4ff7748549392",
      "8dd7421380e64b86aada4e7b4a09b15b",
      "eaa2b1d6b5a64c2a8050a7888b3c4648",
      "fce99cbf870c41ffa3460cd8f6fd665e",
      "0e62afd9b85a4004878c63dfc26c641f",
      "c1c0ab070f2448fe804693456aa6c679",
      "9e3e3179c27e42698c442afaca46eb05",
      "ede421717dd5461183db3a6af60d40f3",
      "59de36b98d12457182bb85fada4e53ed",
      "fafdec94b3e147bba0d999342e98f550",
      "5cb65bde9e8242e38aa98862e05208a0",
      "eb0549767ae94f649a9cbb15abe61bc9",
      "62e623c653c6413dba5cf0f0f24b4faf",
      "58034845c96348bc89df363fcf344da6",
      "ab2c635efd8644a985b6720f5a102c96",
      "25efba438e33473da11ac3a7e3a793e6",
      "100b019c5a214cb8b85b7f5ae2119d5d",
      "197a694dd64d41348475168098abb3ab",
      "be8eba78342c44bf90bfb62b68a4bf09",
      "f9367da780a94ae2860863852c9515bc",
      "f686ebfdfa0e452081e44a191017aab6",
      "66e279d8f1ff4be9a3892cee6e700959",
      "f216321cede6408ba0ff90c0b52ece4f",
      "8813817abcf7487e8b83a7f5c6055f95",
      "f2d75fc6327243208c3740332d054637",
      "0e4b3f9b418c470c8e3a7e939e9c40c3",
      "ba00e694bbba41f5b8045352ad057f97",
      "c40e2bc874c34305bc5d8920310f7d94",
      "aae8d86cb83c45d4a15e9ec768ce6de4",
      "04576ed4663c4754bfc64ee8bd60898e",
      "215c69a650fa447e90aa977a24fb4dc7",
      "9e91d0517db54c74bfe980cebdbb4900",
      "1c1c2d5fbc314f70a120718ff1569241",
      "2982cc146cb747019135164113b63879",
      "064a3daa88a54a96b1275c704b6625d1",
      "33dc62ca03074c52ace954181c28a5f0",
      "5e57ccf6d4064e12be5faf377a51b565",
      "c6d17daa45624f0a9639409b47b94924",
      "5a7a205df38d4adb97875944c5dbdca3",
      "535b6b46bdb54e9dabe5e6e23a40d346",
      "7456cc527a9645068e91bb9e805bbd7e",
      "39bfc699ad3d4e73aabe1234a7ad1573",
      "dddc259c023b4fd88ee640e6d9730e38",
      "81fb844fe2c14e29897ea5b37e672adc",
      "2324c9923fd046a0bc6580e901b92dad",
      "d4d8c992f3364bb5bbfa3c55d0dfb593",
      "83c0fd99eec54214b424512334a5e4a6",
      "900c7b7d5a3b4574bb54a47233bbdaea",
      "c54491ff396146e0999a180e5be4aebf",
      "23f0ea83d72e4ab59d539a567eb20a36",
      "73b78381a5d54cf098a695503727c0cb",
      "f856a6738c404995b68277678818fb7e",
      "1d9cdf6079b648b4a1b8eeb0cea1fa43",
      "2ebbc11da0d54aabacb4add5300b0d08",
      "8b6feda602a040a482cba439f613e7a8",
      "9d9f97aa8106466299752fecd1e7e2d6",
      "6f4f7d14c61b446a851cd93bd17ed003",
      "30ca7c425216497d88967d62d83bc9fd",
      "0d9335d8b2564a518df75d3d0ec8860b",
      "a9bb6750e657475d9373fbc0e7fc85d9",
      "4fe58c1d715748ce8c90e9567a9e69be",
      "2a41774120f94fc2a0a49d467dad9897",
      "4cda3126d0e3428b865dc24d7ae60f2e",
      "a6829eb4b3e64d2caff44b932e63b4c4",
      "d6a2bd5bb29a40528c4d660efdb23e11",
      "9db627ce367242e7a374a14d59419aac",
      "f86f4fd1cc934cdbbfe44f84168646ab",
      "a93dc092082244049dd0589d90cc5d0b",
      "ae40412ec12249719f1a0bcb2650cff4",
      "32526257e6774c15a88f908e4ce3a8f4",
      "5fd4ca07fc894afcbcf55e29809c40d1",
      "f3c41be2d6b54ac68d873432478f6b82",
      "6c5911e019e447ccba3405de971ae416",
      "9e5a03ea3cf9446b8341978d942f7677",
      "37ab0e207b5245608ca7c58b566da4a8",
      "cf8b9e484f29457aa723980d22d1f9db",
      "509f02d8d0f043a9a7eedc4e24fe29c7",
      "ee734f9997e24f64a1a6aec542662fbc",
      "7d810c23b32849e9b38f8b1ad944adbf",
      "f8bbb09e33a84fc4ae2777d08368224f",
      "e9911df32739492c9ae2b7202e5c5899",
      "f4a3cb640ebc42d4a104329f878889a4",
      "1658c20eb4244ce7b578bd4bd52a141f",
      "75ba0f0586e44a7ebb6d67a1dfa75995",
      "0d35ac03b2914b97b1c58d7a24afbca7",
      "b6d5862c271c4118a72b067d53fb2343",
      "6939a02cb7ab40f08486b38f41490aae",
      "2aadcbfcf79840a6ad135b29cb4c9e57",
      "257aad534fcd4bfd9b2a795d54695645",
      "17d5e64c2e0a49aeb88e22746b3ecbac",
      "d9bca499d3674b57bbc50b89ff490d8c",
      "1661a129f1e146828f9003b5b73a247c",
      "241908b8940142dfbd2673ba6abb27a5",
      "1b83a29252b14532867f02c9376cbdc5",
      "9f38bc1f9d81458e9975e67e441da8cc",
      "22d658a6e600477cb704807c10c6408f",
      "156bdbb90bd9404c82751087ae709fae",
      "38224f13f2cb4bb9a315f7e1a6140cb6"
     ]
    },
    "id": "ebaf5ef0",
    "outputId": "2407a1de-5bb4-41e6-e660-202b4d6645db"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 기본 설정\n",
    "max_seq_length = 4096  # 최대 시퀀스 길이\n",
    "dtype = None  # 자동 데이터 타입 감지\n",
    "\n",
    "# 고속 다운로드 지원 4비트 사전 양자화 모델 목록\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",  # Instruct 버전\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",  # Instruct 버전\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # TTS 모델 지원\n",
    "]  \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-14B\",  # 모델명\n",
    "    max_seq_length=max_seq_length,  # 시퀀스 길이\n",
    "    dtype=dtype,  # 데이터 타입\n",
    "    load_in_4bit=True,  # 4비트 양자화\n",
    "    load_in_8bit=False,  # 8비트 양자화\n",
    "    full_finetuning = False,  # 전체 파인튜닝 비활성화\n",
    "    #use_flash_attention=True,  # 플래시 어텐션\n",
    "    # token = \"hf_...\",  # 게이트 모델용 토큰\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e37843",
   "metadata": {},
   "source": [
    "## 3. LoRA 어댑터 구성\n",
    "\n",
    "LoRA 어댑터를 추가하여 모든 파라미터 중 단 1% ~ 10%의 파라미터만 업데이트\n",
    "\n",
    "FastLanguageModel을 사용하여 특정 모듈에 대한 성능 향상 기법을 적용한 모델을 구성합니다.\n",
    "\n",
    "- `FastLanguageModel.get_peft_model` 함수를 호출하여 모델을 초기화하고, 성능 향상을 위한 여러 파라미터를 설정합니다.\n",
    "- `r` 파라미터를 통해 성능 향상 기법의 강도를 조절합니다. 권장 값으로는 8, 16, 32, 64, 128 등이 있습니다.\n",
    "- `target_modules` 리스트에는 성능 향상을 적용할 모델의 모듈 이름들이 포함됩니다.\n",
    "- `lora_alpha`와 `lora_dropout`을 설정하여 LoRA(Low-Rank Adaptation) 기법의 세부 파라미터를 조정합니다.\n",
    "- `bias` 옵션을 통해 모델의 바이어스 사용 여부를 설정할 수 있으며, 최적화를 위해 \"none\"으로 설정하는 것이 권장됩니다.\n",
    "- `use_gradient_checkpointing` 옵션을 \"unsloth\"로 설정하여 VRAM 사용량을 줄이고, 더 큰 배치 크기로 학습할 수 있도록 합니다.\n",
    "- `use_rslora` 옵션을 통해 Rank Stabilized LoRA를 사용할지 여부를 결정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f990387e",
    "outputId": "663b2111-d856-4e9c-999a-debdc08ca49e"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.\n",
    "    lora_alpha=32,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout=0.05,  # 드롭아웃을 지원합니다. Supports any, but = 0 is optimized\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # 타겟 모듈을 지정합니다.\n",
    "    bias=\"none\",  # 바이어스를 지원합니다.  but = \"none\" is optimized\n",
    "    \n",
    "    use_gradient_checkpointing=\"unsloth\",# 메모리 절약을 위해 그래디언트 체크포인트를 사용합니다. True 또는 \"unsloth\"를 사용하여 매우 긴 컨텍스트에 대해 VRAM을 30% 덜 사용하고, 2배 더 큰 배치 크기를 지원합니다. \n",
    "    random_state=123,  # 난수 상태를 설정합니다.\n",
    "    use_rslora=False,  # 순위 안정화 LoRA를 지원합니다.\n",
    "    loftq_config=None,  # LoftQ를 지원합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "1cd96e8b"
   },
   "source": [
    "## 4. 학습 데이터 준비\n",
    "\n",
    "모델 학습을 위해 QA 데이터셋을 적절한 형식으로 준비합니다.\n",
    "\n",
    "- **데이터셋 로드**: `load_dataset` 함수를 통해 Q&A 학습용 데이터셋의 \"train\" 분할 불러오기\n",
    "- **포맷 변환**: 각 예제를 모델 학습에 적합한 형식으로 변환\n",
    "\n",
    "### 포맷팅 프로세스\n",
    "1. `formatting_prompts_func` 함수가 \"instruction\"과 \"output\" 필드를 처리\n",
    "2. 데이터를 알파카(Alpaca) 형식으로 구조화\n",
    "3. 각 항목 끝에 `EOS_TOKEN` 추가하여 생성 종료 지점 명시\n",
    "\n",
    "#### 결과 형식\n",
    "```python\n",
    "{\n",
    "    \"text\": f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}{EOS_TOKEN}\"\n",
    "}\n",
    "```\n",
    "\n",
    "이 전처리 과정을 통해 AI 모델이 효과적으로 학습할 수 있는 표준화된 데이터 형식이 생성됩니다.\n",
    "\n",
    "**[중요]**\n",
    "\n",
    "- llama모델의 경우, 토큰화된 출력에 **EOS_TOKEN**을 추가하는 것을 잊지 마세요! 그렇지 않으면 무한 생성이 발생할 수 있습니다.\n",
    "\n",
    "**[참고]**\n",
    "\n",
    "- 오직 완성된 텍스트만을 학습하고자 한다면, TRL의 문서를 [여기](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)에서 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1051a18ef1a94646a330d1f42b33afc4",
      "3c0f97331ac54e4096419842d32e4e7e",
      "d23634e1e21941c6a4631e1c24b9542e",
      "694947e5188942d1b2d112d82177d1b0",
      "2ae31dabcf524a3ca132fb6a6f6c52d0",
      "e2bf55927a7d433fada252635594dff5",
      "0683156f8cee42b38ccf22a3522461ec",
      "8f7d53d84b1b4884b300e95ef65b251a",
      "872f2ce0bc8745d89d2b0dfc76cebeda",
      "7238acef7a784aec9ac2ad6942c34ff0",
      "6ba81f98392e47a1bc840362e1c56939",
      "0c35cb640bec45b3beb10e7c36ec41ae",
      "f38b82466bff4e9e9691717840b08f81",
      "fe145b3cf6cb4916aec92c855ac6f5cc",
      "8bc57e7f304f475ba344b4d44857c581",
      "e75e8e176da44cafbac169630481269e",
      "343d5b87f7e846258d1c4242ae979a07",
      "3db5f7d69a6e41e98d4bcc67b92f1ff5",
      "14e2b8cd41764e2b9f0a2a75c5e7c22c",
      "7ec19800bab64e3db16c132439dc01fe",
      "d49c61f792284b1f92b421b9f79d3ad1",
      "217ab9f097954ba5975bff834f78a597",
      "1320c8b175874ceab90b3080c055578b",
      "03f2d478a6f04a4ba73a1ab0bfbfcbe9",
      "51a00fa35cf642fd8090298cf82fbe28",
      "9bff62df789f4675a2790a4708558e71",
      "c2c74e7cc99a40d69bc66a2ca0735dec",
      "d67ce9b974ad4a3987782906ae9ae0fc",
      "84c11a93eec642ee84549e837e9564f5",
      "f89378aef63148b290f9c33b36f0265f",
      "f818588783354dffa838660e33367672",
      "4ea84d4aeddb418987748906f8d89104",
      "e6c1e674d7794cd1ab8294042ac0dfb1",
      "142b69ecaa84447b9e3931c16df041f8",
      "d965b86b78ba46b5bafcfd853c5a1bed",
      "104533e332514cfba5fb43fa4f7d0dbf",
      "044656c5431746e2afa416c0ca91f3ad",
      "4e8c7b53ec5843a5ad18b7fdfdfcdca6",
      "4f2654b42fc84943ba4ea5815a1baa3c",
      "480a9a6f262d43eeb69cd218fb9ab5d6",
      "087a0d727269459198204a6fdca7b445",
      "a62b362e056c4bd2862517e804e22214",
      "059c48277af144f796a94f94e1a9c814",
      "8c654210b9674b959b9570f6d5addbf5"
     ]
    },
    "id": "1eb5d1bf",
    "outputId": "88da0b45-853b-4b14-e95f-6f1019e08390"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 종료 토큰 설정 (무한 생성 방지를 위해 필요)\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# 데이터 포매팅 함수\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = prompt.format(instruction, output) + EOS_TOKEN #여기 추가함.\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL 파일에서 데이터셋 로드 및 포매팅\n",
    "dataset = load_dataset('json', data_files='data/train_data.jsonl', split='train')\n",
    "\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func, \n",
    "    batched=True)\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "4c290b06"
   },
   "source": [
    "## 5. 모델 훈련\n",
    "\n",
    "SFTTrainer를 사용하여 모델을 효율적으로 파인튜닝합니다.\n",
    "\n",
    "- SFTTrainer 참고 문서: [TRL SFT 문서 클릭](https://huggingface.co/docs/trl/sft_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123,
     "referenced_widgets": [
      "ebf0af95b9fc4bb88375dde9df666bb5",
      "9ead35f0817c44e9bd32dc6114ea2eaf",
      "80cf3bbe986049a99fb59772545d12e7",
      "73dabe4081f845e4ac4745c36acf1479",
      "ab9e8aec9cc3469dab2ab1471c90bc8d",
      "8464366c6695491e8527fa649628def1",
      "529c6085364d452fb974758ec6d3d22a",
      "e6877dd00cce4b469fde96f2394069bb",
      "e699c41d7d8547058ced67acbf97da7e",
      "16099d5666cb4765af04f65b9fe6842a",
      "9c0d336467d24b359efd024d204b8e8e"
     ]
    },
    "id": "b41b8fd9",
    "outputId": "173f3598-b388-4f1b-98b6-55d7d9ff554d"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# DataCollator 설정\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # MLM 비활성화\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # 패딩 방향 설정\n",
    "\n",
    "# 모델 학습 설정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # 훈련 모델\n",
    "    tokenizer=tokenizer,  # 사용 토크나이저\n",
    "    train_dataset=dataset,  # 훈련 데이터\n",
    "    eval_dataset=dataset,  # 평가 데이터\n",
    "    dataset_text_field=\"text\",  # 텍스트 필드명\n",
    "    max_seq_length=max_seq_length,  # 최대 길이\n",
    "    dataset_num_proc=2,  # 프로세스 수\n",
    "    packing=False,  # 시퀀스 패킹 비활성화\n",
    "    data_collator=data_collator,  # 필수 데이터 수집기\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,  # 디바이스당 배치 크기\n",
    "        gradient_accumulation_steps=4,  # 그래디언트 누적\n",
    "        warmup_steps=5,  # 웜업 단계\n",
    "        num_train_epochs=3,  # 총 에폭\n",
    "        max_steps=100,  # 최대 단계\n",
    "        do_eval=True,  # 평가 활성화\n",
    "        eval_strategy=\"steps\",  # 평가 전략\n",
    "        logging_steps=1,  # 로깅 간격\n",
    "        learning_rate=2e-4,  # 학습률\n",
    "        fp16=not torch.cuda.is_bf16_supported(),  # fp16 조건부 활성화\n",
    "        bf16=torch.cuda.is_bf16_supported(),  # bf16 조건부 활성화\n",
    "        optim=\"adamw_8bit\",  # 최적화 알고리즘\n",
    "        weight_decay=0.01,  # 가중치 감소율\n",
    "        lr_scheduler_type=\"cosine\",  # 스케줄러 유형\n",
    "        seed=123,  # 랜덤 시드\n",
    "        output_dir=\"outputs\",  # 결과 저장 경로\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "a7e10974"
   },
   "source": [
    "- GPU의 현재 메모리 상태를 확인합니다.\n",
    "- `torch.cuda.get_device_properties(0)`를 사용하여 첫 번째 GPU의 속성을 조회합니다.\n",
    "- `torch.cuda.max_memory_reserved()`를 통해 현재 예약된 최대 메모리를 GB 단위로 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b91c701f",
    "outputId": "2f6e197b-f8e0-451f-e498-b21d6b581c3d"
   },
   "outputs": [],
   "source": [
    "# GPU 메모리 상태 확인\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. 최대 메모리 = {max_memory} GB.\")\n",
    "print(f\"현재 예약된 메모리 = {start_gpu_memory} GB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502a3c8",
   "metadata": {},
   "source": [
    "#### PyTorch 2.3 미만 버전 패치 적용\n",
    "- unsloth_zoo는 PyTorch ≥ 2.3에서 새롭게 추가된 torch.amp.is_autocast_available() API를 호출합니다.  \n",
    "- monkey patch 패치 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker 노트북의 PyTorch 버전이 2.2.2이기 때문에, torch.amp.is_autocast_available() 함수가 없어 AttributeError가 발생\n",
    "# monkey patch 패치 적용\n",
    "# PyTorch 2.3 미만 버전 패치 적용\n",
    "import torch.amp\n",
    "if not hasattr(torch.amp, 'is_autocast_available'):\n",
    "    def is_autocast_available(device_type):\n",
    "        if device_type == 'cuda':\n",
    "            return True\n",
    "        elif device_type == 'cpu':\n",
    "            return hasattr(torch.cpu, 'amp') and hasattr(torch.cpu.amp, 'autocast')\n",
    "        else:\n",
    "            return False\n",
    "    torch.amp.is_autocast_available = is_autocast_available\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b47bd3",
   "metadata": {},
   "source": [
    "### 모델 훈련 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "91622afd",
    "outputId": "f12e2b79-d75e-4b1b-a7c6-0ba3dc065818"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "ac245967"
   },
   "source": [
    "## 6. 훈련 결과 및 메모리 사용량 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f17487f",
    "outputId": "c3cc7749-a144-49b2-a5ab-0a0994193f8b"
   },
   "outputs": [],
   "source": [
    "# 메모리 사용량 및 훈련 시간 통계 계산\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "# 통계 출력\n",
    "print(f\"훈련 시간: {trainer_stats.metrics['train_runtime']} 초 ({round(trainer_stats.metrics['train_runtime']/60, 2)} 분)\")\n",
    "print(f\"최대 사용 메모리: {used_memory} GB (전체의 {used_percentage}%)\")\n",
    "print(f\"LoRA 학습에 사용된 메모리: {used_memory_for_lora} GB (전체의 {lora_percentage}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "bdf87c13"
   },
   "source": [
    "## 7. 모델 추론 및 테스트\n",
    "\n",
    "파인튜닝된 모델을 사용하여 질문에 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90644a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# 생성 중단 조건 설정\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_token_id in input_ids[0] #$ 입력된 ID 중 정지 토큰 ID가 있으면 True를 반환합니다.\n",
    "\n",
    "stop_token = \"<|end_of_text|>\" # end_token으로 사용할 토큰을 설정, 모델에 따라 다를 수 있습니다. llama-3 계열은 <|end_of_text|>를 Qwen3 계열은 <|im_end|>를 사용합니다.\n",
    "\n",
    "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[0]\n",
    "stopping_criteria = StoppingCriteriaList([StopOnToken(stop_token_id)]) #    정지 조건을 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc37f7",
   "metadata": {},
   "source": [
    "#### 추론 모드 설정 (2배 빠른 생성 속도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6500b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "_ZGu2HntqpdN"
   },
   "source": [
    "#### 추론 예시 1: 학습 테스트 더미 확인용 인물 정보 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f8441b0",
    "outputId": "380dc31c-4d36-4ee0-a32e-891ec36236aa"
   },
   "outputs": [],
   "source": [
    "# 텍스트 스트리밍 설정\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# 질문 입력 및 응답 생성\n",
    "inputs = tokenizer(\n",
    "    [prompt.format(\"전현상은 누구입니까?\", \"\")],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 생성 실행\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=1024,\n",
    "    stopping_criteria=stopping_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecce662",
   "metadata": {},
   "source": [
    "#### 추론 예시 2: 주소 정보 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 입력 및 응답 생성\n",
    "inputs = tokenizer(\n",
    "    [prompt.format(\"금융보안교육센터의 주소?\", \"\")],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 생성 실행\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=1024,\n",
    "    stopping_criteria=stopping_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "emWg-dCBqjZD"
   },
   "source": [
    "(예시2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
